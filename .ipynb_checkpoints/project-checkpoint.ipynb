{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9df8ee-2c42-42d1-a4f2-f0a31df957c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            accuracy_score, f1_score, precision_recall_fscore_support)\n",
    "from scipy.sparse import hstack\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import joblib\n",
    "\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'data': Path('output/data'),\n",
    "    'models': Path('output/models'),\n",
    "    'plots': Path('output/plots'),\n",
    "    'reports': Path('output/reports')\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f4b65-b7bb-4567-8738-b81fbefad0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nltk_data():\n",
    "    \"\"\"Download required NLTK datasets.\"\"\"\n",
    "    required = ['stopwords', 'punkt', 'punkt_tab', 'wordnet', \n",
    "                'averaged_perceptron_tagger', 'omw-1.4']\n",
    "    \n",
    "    print(\"Downloading NLTK data...\", end=' ')\n",
    "    for item in required:\n",
    "        nltk.download(item, quiet=True)\n",
    "    print(\"✓\")\n",
    "\n",
    "def save_plot(filename, subdir='plots'):\n",
    "    \"\"\"\n",
    "    Save current plot to output directory and display in notebook.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of file to save\n",
    "        subdir: Subdirectory within plots folder\n",
    "    \"\"\"\n",
    "    save_path = OUTPUT_DIRS['plots'] / subdir\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    plt.savefig(save_path / filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def load_amazon_reviews(file_path, max_rows=None):\n",
    "    \"\"\"\n",
    "    Load Amazon reviews from compressed JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to .jsonl.gz file\n",
    "        max_rows: Optional limit on number of reviews to load\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing all review data\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    \n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_rows and i >= max_rows:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                review = json.loads(line.strip())\n",
    "                reviews.append(review)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            \n",
    "            if (i + 1) % 100000 == 0:\n",
    "                print(f\"  Progress: {i + 1:,} reviews\", end='\\r')\n",
    "    \n",
    "    print(f\"\\n✓ Loaded {len(reviews):,} reviews\")\n",
    "    return pd.DataFrame(reviews)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize review text.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw review text\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize, remove stopwords, and lemmatize text.\n",
    "    \n",
    "    Args:\n",
    "        text: Cleaned text string\n",
    "    \n",
    "    Returns:\n",
    "        List of processed tokens\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keep_words = {'not', 'no', 'never', 'neither', 'nobody', 'nothing', \n",
    "                  'nowhere', 'hardly', 'barely', 'scarcely', \"don't\", \"doesn't\",\n",
    "                  \"didn't\", \"won't\", \"wouldn't\", \"shouldn't\", \"couldn't\", \"can't\"}\n",
    "    stop_words = stop_words - keep_words\n",
    "    \n",
    "    tokens = [w for w in tokens if len(w) > 2 and w not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def create_sentiment_label(rating):\n",
    "    \"\"\"\n",
    "    Convert numeric rating to sentiment category.\n",
    "    \n",
    "    Args:\n",
    "        rating: Numeric rating (1-5)\n",
    "    \n",
    "    Returns:\n",
    "        Sentiment label: 'negative', 'neutral', or 'positive'\n",
    "    \"\"\"\n",
    "    if rating <= 2:\n",
    "        return 'negative'\n",
    "    elif rating == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit=20, start=5, step=1):\n",
    "    \"\"\"\n",
    "    Compute coherence scores for different numbers of topics.\n",
    "    \n",
    "    Args:\n",
    "        dictionary: Gensim dictionary\n",
    "        corpus: Gensim corpus\n",
    "        texts: List of tokenized documents\n",
    "        limit: Maximum number of topics to test\n",
    "        start: Minimum number of topics to test\n",
    "        step: Step size for topic range\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model_list, coherence_values)\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(f\"  Testing {num_topics} topics...\", end='\\r')\n",
    "        \n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            random_state=42,\n",
    "            chunksize=2000,\n",
    "            passes=10,\n",
    "            alpha='auto',\n",
    "            eta='auto',\n",
    "            per_word_topics=True,\n",
    "            eval_every=None\n",
    "        )\n",
    "        \n",
    "        model_list.append(model)\n",
    "        \n",
    "        coherencemodel = CoherenceModel(\n",
    "            model=model,\n",
    "            texts=texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "        coherence = coherencemodel.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "    \n",
    "    print(f\"\\n✓ Tested {len(model_list)} topic configurations\")\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def get_document_topics(lda_model, corpus):\n",
    "    \"\"\"\n",
    "    Extract topic probabilities for each document in corpus.\n",
    "    \n",
    "    Args:\n",
    "        lda_model: Trained LDA model\n",
    "        corpus: Gensim corpus\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with topic assignments and probabilities\n",
    "    \"\"\"\n",
    "    all_topics = []\n",
    "    \n",
    "    for i, doc_topics in enumerate(lda_model[corpus]):\n",
    "        if (i + 1) % 50000 == 0:\n",
    "            print(f\"  Processed {i + 1:,} reviews\", end='\\r')\n",
    "        \n",
    "        topic_probs = dict(doc_topics[0])\n",
    "        \n",
    "        if topic_probs:\n",
    "            dominant_topic = max(topic_probs.items(), key=lambda x: x[1])\n",
    "            \n",
    "            topic_dict = {\n",
    "                'dominant_topic': dominant_topic[0],\n",
    "                'dominant_prob': dominant_topic[1]\n",
    "            }\n",
    "            \n",
    "            for topic_id in range(lda_model.num_topics):\n",
    "                topic_dict[f'topic_{topic_id}_prob'] = topic_probs.get(topic_id, 0.0)\n",
    "            \n",
    "            all_topics.append(topic_dict)\n",
    "        else:\n",
    "            all_topics.append({\n",
    "                'dominant_topic': -1,\n",
    "                'dominant_prob': 0.0\n",
    "            })\n",
    "    \n",
    "    print(\"\\n✓ Topic assignment complete\")\n",
    "    return pd.DataFrame(all_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388f7ad-739c-4779-b057-91aafcd7ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_exploration(df):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data exploration visualizations.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with review data\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    df['rating'].value_counts().sort_index().plot(kind='bar', ax=axes[0, 0], color='steelblue')\n",
    "    axes[0, 0].set_title('Rating Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Rating')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].hist(df['word_count'], bins=50, color='coral', edgecolor='black')\n",
    "    axes[0, 1].set_title('Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Number of Words')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_xlim(0, 500)\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df['year_month'] = df['date'].dt.to_period('M')\n",
    "    reviews_over_time = df['year_month'].value_counts().sort_index()\n",
    "    reviews_over_time.plot(ax=axes[1, 0], color='green')\n",
    "    axes[1, 0].set_title('Reviews Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Date')\n",
    "    axes[1, 0].set_ylabel('Number of Reviews')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    verified_counts = df['verified_purchase'].value_counts()\n",
    "    axes[1, 1].pie(verified_counts, labels=['Verified', 'Unverified'], \n",
    "                   autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "    axes[1, 1].set_title('Verified vs Unverified Purchases', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot('data_exploration.png', 'exploration')\n",
    "\n",
    "def plot_topic_coherence(num_topics, coherence_values):\n",
    "    \"\"\"\n",
    "    Plot coherence scores for different numbers of topics.\n",
    "    \n",
    "    Args:\n",
    "        num_topics: List of topic numbers tested\n",
    "        coherence_values: Corresponding coherence scores\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(num_topics, coherence_values, marker='o', linewidth=2, markersize=8)\n",
    "    plt.xlabel(\"Number of Topics\", fontsize=12)\n",
    "    plt.ylabel(\"Coherence Score\", fontsize=12)\n",
    "    plt.title(\"Topic Coherence Scores\", fontsize=14, fontweight='bold')\n",
    "    plt.xticks(num_topics)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot('topic_coherence_scores.png', 'lda')\n",
    "\n",
    "def plot_confusion_matrix_comparison(y_test, y_pred_before, y_pred_after):\n",
    "    \"\"\"\n",
    "    Compare confusion matrices before and after SMOTE.\n",
    "    \n",
    "    Args:\n",
    "        y_test: True labels\n",
    "        y_pred_before: Predictions before balancing\n",
    "        y_pred_after: Predictions after balancing\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "    label_names = ['Negative', 'Neutral', 'Positive']\n",
    "    \n",
    "    cm_before = confusion_matrix(y_test, y_pred_before, labels=labels)\n",
    "    sns.heatmap(cm_before, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                xticklabels=label_names, yticklabels=label_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    axes[0].set_title('Before SMOTE (Imbalanced)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('True Label', fontsize=12)\n",
    "    axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    cm_after = confusion_matrix(y_test, y_pred_after, labels=labels)\n",
    "    sns.heatmap(cm_after, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "                xticklabels=label_names, yticklabels=label_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    axes[1].set_title('After SMOTE (Balanced)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('True Label', fontsize=12)\n",
    "    axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot('confusion_matrix_comparison.png', 'classification')\n",
    "\n",
    "def plot_f1_comparison(before_metrics, after_metrics):\n",
    "    \"\"\"\n",
    "    Compare F1 scores before and after SMOTE.\n",
    "    \n",
    "    Args:\n",
    "        before_metrics: Metrics tuple before balancing\n",
    "        after_metrics: Metrics tuple after balancing\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = np.arange(3)\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, before_metrics[2], width, \n",
    "                   label='Before SMOTE', color='lightcoral')\n",
    "    bars2 = ax.bar(x + width/2, after_metrics[2], width, \n",
    "                   label='After SMOTE', color='lightgreen')\n",
    "    \n",
    "    ax.set_xlabel('Sentiment Class', fontsize=12)\n",
    "    ax.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax.set_title('F1 Score Comparison: Before vs After SMOTE', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.3f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot('f1_score_comparison.png', 'classification')\n",
    "\n",
    "def plot_aspect_sentiment_distribution(df):\n",
    "    \"\"\"\n",
    "    Visualize sentiment distribution across product aspects.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with topic_label and predicted_sentiment\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
    "    \n",
    "    aspect_sentiment_counts = pd.crosstab(df['topic_label'], df['predicted_sentiment'])\n",
    "    aspect_sentiment_pct = pd.crosstab(df['topic_label'], df['predicted_sentiment'], \n",
    "                                       normalize='index') * 100\n",
    "    \n",
    "    aspect_sentiment_counts.plot(kind='bar', stacked=False, ax=axes[0], \n",
    "                                 color=['#d62728', '#ff7f0e', '#2ca02c'])\n",
    "    axes[0].set_title('Sentiment Distribution by Aspect (Counts)', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Product Aspect', fontsize=12)\n",
    "    axes[0].set_ylabel('Number of Reviews', fontsize=12)\n",
    "    axes[0].legend(title='Sentiment', labels=['Negative', 'Neutral', 'Positive'])\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    aspect_sentiment_pct.plot(kind='bar', stacked=False, ax=axes[1],\n",
    "                              color=['#d62728', '#ff7f0e', '#2ca02c'])\n",
    "    axes[1].set_title('Sentiment Distribution by Aspect (%)', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Product Aspect', fontsize=12)\n",
    "    axes[1].set_ylabel('Percentage', fontsize=12)\n",
    "    axes[1].legend(title='Sentiment', labels=['Negative', 'Neutral', 'Positive'])\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot('aspect_sentiment_distribution.png', 'aspect_analysis')\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(aspect_sentiment_counts, annot=True, fmt='d', cmap='RdYlGn',\n",
    "                cbar_kws={'label': 'Number of Reviews'})\n",
    "    plt.title('Aspect-Based Sentiment Analysis Heatmap', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Sentiment', fontsize=12)\n",
    "    plt.ylabel('Product Aspect', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    save_plot('absa_heatmap.png', 'aspect_analysis')\n",
    "\n",
    "def generate_aspect_wordclouds(df, aspect_name):\n",
    "    \"\"\"\n",
    "    Generate word clouds for each sentiment within a product aspect.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with processed_text and predicted_sentiment\n",
    "        aspect_name: Name of the aspect to analyze\n",
    "    \"\"\"\n",
    "    aspect_df = df[df['topic_label'] == aspect_name]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    sentiments = ['positive', 'neutral', 'negative']\n",
    "    colors = ['Greens', 'Blues', 'Reds']\n",
    "    \n",
    "    for ax, sentiment, colormap in zip(axes, sentiments, colors):\n",
    "        text_data = aspect_df[aspect_df['predicted_sentiment'] == sentiment]['processed_text']\n",
    "        \n",
    "        if len(text_data) > 0:\n",
    "            text = ' '.join(text_data.values)\n",
    "            \n",
    "            wordcloud = WordCloud(\n",
    "                width=600, height=400,\n",
    "                background_color='white',\n",
    "                colormap=colormap,\n",
    "                max_words=50,\n",
    "                relative_scaling=0.5\n",
    "            ).generate(text)\n",
    "            \n",
    "            ax.imshow(wordcloud, interpolation='bilinear')\n",
    "            ax.set_title(f'{sentiment.capitalize()} ({len(text_data)} reviews)', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No reviews', ha='center', va='center')\n",
    "            ax.set_title(f'{sentiment.capitalize()} (0 reviews)', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "        \n",
    "        ax.axis('off')\n",
    "    \n",
    "    fig.suptitle(f'Word Clouds: {aspect_name}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    filename = f'wordcloud_{aspect_name.replace(\" \", \"_\").replace(\"&\", \"and\").lower()}.png'\n",
    "    save_plot(filename, 'aspect_analysis/wordclouds')\n",
    "\n",
    "def plot_sentiment_trends(df, aspect_name=None):\n",
    "    \"\"\"\n",
    "    Plot sentiment trends over time for overall or specific aspect.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with year_month and predicted_sentiment\n",
    "        aspect_name: Optional aspect name to filter by\n",
    "    \"\"\"\n",
    "    if aspect_name:\n",
    "        df_plot = df[df['topic_label'] == aspect_name]\n",
    "        title = f'Sentiment Trend: {aspect_name}'\n",
    "        filename = f'sentiment_trend_{aspect_name.replace(\" \", \"_\").replace(\"&\", \"and\").lower()}.png'\n",
    "    else:\n",
    "        df_plot = df\n",
    "        title = 'Overall Sentiment Trends Over Time'\n",
    "        filename = 'sentiment_trend_overall.png'\n",
    "    \n",
    "    sentiment_over_time = df_plot.groupby(['year_month', 'predicted_sentiment']).size().unstack(fill_value=0)\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sentiment_over_time.plot(kind='line', marker='o', ax=plt.gca(),\n",
    "                             color=['#d62728', '#ff7f0e', '#2ca02c'])\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Number of Reviews', fontsize=12)\n",
    "    plt.legend(title='Sentiment', labels=['Negative', 'Neutral', 'Positive'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot(filename, 'aspect_analysis/trends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54512ab3-ec58-47e3-a338-1f44f992efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(X_train, y_train, target_positive=150000, \n",
    "                   target_negative=100000, target_neutral=80000):\n",
    "    \"\"\"\n",
    "    Balance dataset using SMOTE oversampling and random undersampling.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        target_positive: Target count for positive class\n",
    "        target_negative: Target count for negative class\n",
    "        target_neutral: Target count for neutral class\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_balanced, y_balanced)\n",
    "    \"\"\"\n",
    "    print(\"\\n  Applying SMOTE...\", end=' ')\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy={\n",
    "            'neutral': target_neutral,\n",
    "            'negative': target_negative\n",
    "        },\n",
    "        random_state=42,\n",
    "        k_neighbors=5\n",
    "    )\n",
    "    X_over, y_over = smote.fit_resample(X_train, y_train)\n",
    "    print(\"✓\")\n",
    "    \n",
    "    print(\"  Applying undersampling...\", end=' ')\n",
    "    undersample = RandomUnderSampler(\n",
    "        sampling_strategy={'positive': target_positive},\n",
    "        random_state=42\n",
    "    )\n",
    "    X_balanced, y_balanced = undersample.fit_resample(X_over, y_over)\n",
    "    print(\"✓\")\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple Naive Bayes models.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with model results\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'MultinomialNB': MultinomialNB(alpha=0.1),\n",
    "        'ComplementNB': ComplementNB(alpha=0.1)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"  Training {name}...\", end=' ')\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        print(f\"✓ (F1: {f1:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def tune_hyperparameters(ModelClass, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform grid search for optimal hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        ModelClass: Naive Bayes model class\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "        Tuned model\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0],\n",
    "        'fit_prior': [True, False]\n",
    "    }\n",
    "    \n",
    "    print(\"  Performing grid search...\", end=' ')\n",
    "    grid_search = GridSearchCV(\n",
    "        ModelClass(),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"✓ (Best F1: {grid_search.best_score_:.4f})\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def analyze_aspect(df, aspect_name, n_examples=2):\n",
    "    \"\"\"\n",
    "    Perform detailed analysis of a specific product aspect.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with review data\n",
    "        aspect_name: Name of aspect to analyze\n",
    "        n_examples: Number of example reviews to show per sentiment\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame filtered to specified aspect\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ASPECT: {aspect_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    aspect_df = df[df['topic_label'] == aspect_name].copy()\n",
    "    \n",
    "    print(f\"\\nTotal reviews: {len(aspect_df):,}\")\n",
    "    \n",
    "    sentiment_counts = aspect_df['predicted_sentiment'].value_counts()\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    for sentiment in ['positive', 'neutral', 'negative']:\n",
    "        if sentiment in sentiment_counts.index:\n",
    "            count = sentiment_counts[sentiment]\n",
    "            pct = (count / len(aspect_df)) * 100\n",
    "            print(f\"  {sentiment.capitalize():<10}: {count:>6,} ({pct:>5.1f}%)\")\n",
    "    \n",
    "    avg_rating = aspect_df['rating'].mean()\n",
    "    print(f\"\\nAverage Rating: {avg_rating:.2f}/5.0\")\n",
    "    \n",
    "    for sentiment in ['positive', 'negative', 'neutral']:\n",
    "        examples = aspect_df[aspect_df['predicted_sentiment'] == sentiment].head(n_examples)\n",
    "        \n",
    "        if len(examples) > 0:\n",
    "            print(f\"\\n{sentiment.upper()} EXAMPLES:\")\n",
    "            for idx, (_, row) in enumerate(examples.iterrows(), 1):\n",
    "                print(f\"\\n  [{idx}] Rating: {row['rating']:.0f}/5 | Confidence: {row['dominant_prob']:.2f}\")\n",
    "                text = row['text'] if len(row['text']) <= 200 else row['text'][:200] + \"...\"\n",
    "                print(f\"      \\\"{text}\\\"\")\n",
    "    \n",
    "    return aspect_df\n",
    "\n",
    "def save_smote_report(y_train, y_train_balanced, y_test, y_pred_before, y_pred_after):\n",
    "    \"\"\"\n",
    "    Generate and save comprehensive SMOTE improvement report.\n",
    "    \n",
    "    Args:\n",
    "        y_train: Original training labels\n",
    "        y_train_balanced: Balanced training labels\n",
    "        y_test: Test labels\n",
    "        y_pred_before: Predictions before balancing\n",
    "        y_pred_after: Predictions after balancing\n",
    "    \"\"\"\n",
    "    report_path = OUTPUT_DIRS['reports'] / 'smote_improvement_report.txt'\n",
    "    \n",
    "    before_metrics = precision_recall_fscore_support(y_test, y_pred_before, \n",
    "                                                       labels=['negative', 'neutral', 'positive'])\n",
    "    after_metrics = precision_recall_fscore_support(y_test, y_pred_after,\n",
    "                                                      labels=['negative', 'neutral', 'positive'])\n",
    "    \n",
    "    improvement_df = pd.DataFrame({\n",
    "        'Class': ['Negative', 'Neutral', 'Positive'],\n",
    "        'F1_Before': before_metrics[2],\n",
    "        'F1_After': after_metrics[2],\n",
    "        'F1_Improvement': after_metrics[2] - before_metrics[2],\n",
    "        'Recall_Before': before_metrics[1],\n",
    "        'Recall_After': after_metrics[1],\n",
    "        'Recall_Improvement': after_metrics[1] - before_metrics[1]\n",
    "    })\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        f.write(\"SMOTE IMPROVEMENT REPORT\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"ORIGINAL DATA DISTRIBUTION:\\n\")\n",
    "        f.write(f\"  Positive: {(y_train == 'positive').sum():,} ({(y_train == 'positive').mean()*100:.1f}%)\\n\")\n",
    "        f.write(f\"  Negative: {(y_train == 'negative').sum():,} ({(y_train == 'negative').mean()*100:.1f}%)\\n\")\n",
    "        f.write(f\"  Neutral: {(y_train == 'neutral').sum():,} ({(y_train == 'neutral').mean()*100:.1f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"BALANCED DATA DISTRIBUTION:\\n\")\n",
    "        f.write(f\"  Positive: {(y_train_balanced == 'positive').sum():,}\\n\")\n",
    "        f.write(f\"  Negative: {(y_train_balanced == 'negative').sum():,}\\n\")\n",
    "        f.write(f\"  Neutral: {(y_train_balanced == 'neutral').sum():,}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PERFORMANCE COMPARISON:\\n\")\n",
    "        f.write(f\"  Before SMOTE - Weighted F1: {f1_score(y_test, y_pred_before, average='weighted'):.4f}\\n\")\n",
    "        f.write(f\"  After SMOTE  - Weighted F1: {f1_score(y_test, y_pred_after, average='weighted'):.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PER-CLASS F1 IMPROVEMENTS:\\n\")\n",
    "        f.write(improvement_df.to_string(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"BEFORE SMOTE:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred_before))\n",
    "        f.write(\"\\n\\nAFTER SMOTE:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred_after))\n",
    "    \n",
    "    print(f\"✓ Report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c21af-d693-44ac-a850-12bf44289d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    \"\"\"Download NLTK data and create directory structure.\"\"\"\n",
    "    download_nltk_data()\n",
    "    print(\"✓ Setup complete\\n\")\n",
    "\n",
    "def load_and_explore(file_path='All_Beauty.jsonl.gz'):\n",
    "    \"\"\"Load and perform initial exploration of review data.\"\"\"\n",
    "    \n",
    "    print(\"\\nLoading reviews...\")\n",
    "    df = load_amazon_reviews(file_path)\n",
    "    \n",
    "    df.to_csv(OUTPUT_DIRS['data'] / 'all_beauty_reviews.csv', index=False)\n",
    "    print(f\"✓ Saved to {OUTPUT_DIRS['data'] / 'all_beauty_reviews.csv'}\")\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    print(f\"\\nTotal reviews: {len(df):,}\")\n",
    "    print(f\"Unique users: {df['user_id'].nunique():,}\")\n",
    "    print(f\"Unique products: {df['parent_asin'].nunique():,}\")\n",
    "    print(f\"Verified purchases: {df['verified_purchase'].sum():,} ({df['verified_purchase'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nRating distribution:\")\n",
    "    print(df['rating'].value_counts().sort_index())\n",
    "    \n",
    "    df['text_length'] = df['text'].fillna('').str.len()\n",
    "    df['word_count'] = df['text'].fillna('').str.split().str.len()\n",
    "    \n",
    "    print(f\"\\nAverage text length: {df['text_length'].mean():.0f} characters\")\n",
    "    print(f\"Average word count: {df['word_count'].mean():.0f} words\")\n",
    "    \n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    plot_data_exploration(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df=None):\n",
    "    \"\"\"Clean and preprocess review text.\"\"\"\n",
    "\n",
    "    if df is None:\n",
    "        print(\"\\nLoading data...\")\n",
    "        df = pd.read_csv(OUTPUT_DIRS['data'] / 'all_beauty_reviews.csv')\n",
    "    \n",
    "    print(f\"\\nOriginal dataset: {len(df):,} reviews\")\n",
    "    \n",
    "    df = df[df['text'].notna()].copy()\n",
    "    print(f\"After removing null text: {len(df):,} reviews\")\n",
    "    \n",
    "    print(\"\\nCleaning text...\")\n",
    "    df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    print(\"Tokenizing and lemmatizing...\")\n",
    "    df['tokens'] = df['cleaned_text'].apply(preprocess_text)\n",
    "    df['processed_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    df['token_count'] = df['tokens'].apply(len)\n",
    "    df = df[df['token_count'] >= 5].copy()\n",
    "    \n",
    "    print(f\"After filtering short reviews: {len(df):,} reviews\")\n",
    "    \n",
    "    df['sentiment'] = df['rating'].apply(create_sentiment_label)\n",
    "    \n",
    "    df.to_csv(OUTPUT_DIRS['data'] / 'all_beauty_preprocessed.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"PREPROCESSING COMPLETE\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Final dataset: {len(df):,} reviews\")\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "    print(f\"\\n✓ Saved to {OUTPUT_DIRS['data'] / 'all_beauty_preprocessed.csv'}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def find_optimal_topics(df=None, sample_size=100000):\n",
    "    \"\"\"Find optimal number of topics using coherence scores.\"\"\"\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\nLoading preprocessed data...\")\n",
    "        df = pd.read_csv(OUTPUT_DIRS['data'] / 'all_beauty_preprocessed.csv')\n",
    "        df['tokens'] = df['tokens'].apply(eval)\n",
    "    \n",
    "    print(f\"Dataset size: {len(df):,} reviews\")\n",
    "    \n",
    "    if len(df) > sample_size:\n",
    "        print(f\"Sampling {sample_size:,} reviews for coherence testing...\")\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    else:\n",
    "        df_sample = df.copy()\n",
    "    \n",
    "    print(\"\\nCreating dictionary and corpus...\")\n",
    "    dictionary = corpora.Dictionary(df_sample['tokens'])\n",
    "    \n",
    "    print(f\"Dictionary size before filtering: {len(dictionary):,}\")\n",
    "    \n",
    "    dictionary.filter_extremes(\n",
    "        no_below=10,\n",
    "        no_above=0.5,\n",
    "        keep_n=5000\n",
    "    )\n",
    "    \n",
    "    print(f\"Dictionary size after filtering: {len(dictionary):,}\")\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in df_sample['tokens']]\n",
    "    \n",
    "    dictionary.save(str(OUTPUT_DIRS['models'] / 'beauty_dictionary.dict'))\n",
    "    with open(OUTPUT_DIRS['models'] / 'beauty_corpus.pkl', 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"FINDING OPTIMAL NUMBER OF TOPICS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    model_list, coherence_values = compute_coherence_values(\n",
    "        dictionary=dictionary,\n",
    "        corpus=corpus,\n",
    "        texts=df_sample['tokens'].tolist(),\n",
    "        start=5,\n",
    "        limit=16,\n",
    "        step=1\n",
    "    )\n",
    "    \n",
    "    num_topics = list(range(5, 16, 1))\n",
    "    plot_topic_coherence(num_topics, coherence_values)\n",
    "    \n",
    "    optimal_idx = coherence_values.index(max(coherence_values))\n",
    "    optimal_topics = num_topics[optimal_idx]\n",
    "    \n",
    "    print(f\"\\n✓ Optimal number of topics: {optimal_topics}\")\n",
    "    print(f\"✓ Coherence score: {coherence_values[optimal_idx]:.4f}\")\n",
    "    \n",
    "    results = {\n",
    "        'num_topics': num_topics,\n",
    "        'coherence_scores': coherence_values,\n",
    "        'optimal_topics': optimal_topics\n",
    "    }\n",
    "    \n",
    "    with open(OUTPUT_DIRS['models'] / 'coherence_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    return optimal_topics\n",
    "\n",
    "def train_final_lda(df=None):\n",
    "    \"\"\"Train final LDA model with optimal topics.\"\"\"\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"TRAINING FINAL LDA MODEL\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\nLoading data...\")\n",
    "        df = pd.read_csv(OUTPUT_DIRS['data'] / 'all_beauty_preprocessed.csv')\n",
    "        df['tokens'] = df['tokens'].apply(eval)\n",
    "    \n",
    "    dictionary = corpora.Dictionary.load(str(OUTPUT_DIRS['models'] / 'beauty_dictionary.dict'))\n",
    "    \n",
    "    print(\"Creating corpus for full dataset...\")\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]\n",
    "    \n",
    "    with open(OUTPUT_DIRS['models'] / 'coherence_results.pkl', 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "        num_topics = results['optimal_topics']\n",
    "    \n",
    "    print(f\"\\nTraining LDA model with {num_topics} topics...\")\n",
    "    \n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        chunksize=2000,\n",
    "        passes=15,\n",
    "        iterations=400,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        per_word_topics=True,\n",
    "        eval_every=10\n",
    "    )\n",
    "    \n",
    "    lda_model.save(str(OUTPUT_DIRS['models'] / 'beauty_lda_final.model'))\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"DISCOVERED TOPICS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for idx, topic in lda_model.print_topics(num_topics, num_words=15):\n",
    "        print(f\"\\nTopic {idx}:\")\n",
    "        print(f\"  {topic}\")\n",
    "    \n",
    "    topic_labels = {\n",
    "        0: \"General Feedback & Product Experience\", \n",
    "        1: \"Wigs & Hairpieces\",                     \n",
    "        2: \"Usage & Duration\",                      \n",
    "        3: \"Skin Care & Sensitivity\",               \n",
    "        4: \"Product Reviews & Comparisons\",         \n",
    "        5: \"Positive Product Reviews\",              \n",
    "        6: \"Eye Makeup & Tools\",                    \n",
    "        7: \"Product Value & Disappointment\",        \n",
    "        8: \"Cosmetics & Shades\",                    \n",
    "        9: \"Fragrance & Scent\",                     \n",
    "        10: \"Face Masks & Serums\",                  \n",
    "        11: \"Oral Care & Toothbrushes\",             \n",
    "        12: \"Nail Care & Polish\",                   \n",
    "        13: \"Hair Care & Styling\",                  \n",
    "        14: \"Product Fit & Size\",                   \n",
    "    }\n",
    "    \n",
    "    with open(OUTPUT_DIRS['models'] / 'topic_labels.pkl', 'wb') as f:\n",
    "        pickle.dump(topic_labels, f)\n",
    "    \n",
    "    print(\"\\nCreating interactive visualization...\")\n",
    "    vis = gensimvis.prepare(lda_model, corpus, dictionary, mds='mmds')\n",
    "    pyLDAvis.save_html(vis, str(OUTPUT_DIRS['plots'] / 'lda' / 'lda_visualization.html'))\n",
    "    \n",
    "    print(f\"\\n✓ Model saved to {OUTPUT_DIRS['models'] / 'beauty_lda_final.model'}\")\n",
    "    print(f\"✓ Visualization saved to {OUTPUT_DIRS['plots'] / 'lda' / 'lda_visualization.html'}\")\n",
    "    \n",
    "    return lda_model\n",
    "\n",
    "def assign_topics(df=None):\n",
    "    \"\"\"Assign topics to all reviews.\"\"\"\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"ASSIGNING TOPICS TO REVIEWS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\nLoading data...\")\n",
    "        df = pd.read_csv(OUTPUT_DIRS['data'] / 'all_beauty_preprocessed.csv')\n",
    "        df['tokens'] = df['tokens'].apply(eval)\n",
    "    \n",
    "    dictionary = corpora.Dictionary.load(str(OUTPUT_DIRS['models'] / 'beauty_dictionary.dict'))\n",
    "    lda_model = LdaModel.load(str(OUTPUT_DIRS['models'] / 'beauty_lda_final.model'))\n",
    "    \n",
    "    with open(OUTPUT_DIRS['models'] / 'topic_labels.pkl', 'rb') as f:\n",
    "        topic_labels = pickle.load(f)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]\n",
    "    \n",
    "    print(\"\\nProcessing documents...\")\n",
    "    topics_df = get_document_topics(lda_model, corpus)\n",
    "    \n",
    "    df = pd.concat([df.reset_index(drop=True), topics_df], axis=1)\n",
    "    \n",
    "    df['topic_label'] = df['dominant_topic'].map(topic_labels)\n",
    "    df.loc[df['dominant_topic'] == -1, 'topic_label'] = 'Unknown'\n",
    "    \n",
    "    df.to_csv(OUTPUT_DIRS['data'] / 'all_beauty_with_topics.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"TOPIC ASSIGNMENT COMPLETE\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"\\nTopic distribution:\")\n",
    "    print(df['topic_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\n✓ Saved to {OUTPUT_DIRS['data'] / 'all_beauty_with_topics.csv'}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def naive_bayes_classification(df=None):\n",
    "    \"\"\"Train and evaluate Naive Bayes classifier with SMOTE.\"\"\"\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\nLoading data with topics...\")\n",
    "        df = pd.read_csv(OUTPUT_DIRS['data'] / 'all_beauty_with_topics.csv')\n",
    "    \n",
    "    print(f\"Dataset size: {len(df):,}\")\n",
    "    print(\"\\nOriginal sentiment distribution:\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "    print(\"\\nCreating TF-IDF features...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        min_df=5,\n",
    "        max_df=0.7,\n",
    "        ngram_range=(1, 2),\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    \n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "    \n",
    "    topic_prob_cols = [col for col in df.columns if col.startswith('topic_') and col.endswith('_prob')]\n",
    "    X_topics = df[topic_prob_cols].fillna(0).values\n",
    "    \n",
    "    print(\"Combining features...\")\n",
    "    X_combined = hstack([X_tfidf, X_topics])\n",
    "    print(f\"Feature shape: {X_combined.shape}\")\n",
    "    \n",
    "    y = df['sentiment']\n",
    "    \n",
    "    print(\"\\nSplitting data...\")\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "        X_combined, y, df.index,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"BALANCING DATASET WITH SMOTE\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    X_train_balanced, y_train_balanced = balance_dataset(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nBalanced training set: {len(y_train_balanced):,} samples\")\n",
    "    print(pd.Series(y_train_balanced).value_counts())\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"TRAINING MODELS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    results = train_and_evaluate_models(X_train_balanced, y_train_balanced, X_test, y_test)\n",
    "    \n",
    "    best_name = max(results, key=lambda k: results[k]['f1'])\n",
    "    best_model = results[best_name]['model']\n",
    "    BestModelClass = type(best_model)\n",
    "    \n",
    "    print(f\"\\n✓ Best model: {best_name} (F1: {results[best_name]['f1']:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"HYPERPARAMETER TUNING\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    tuned_model = tune_hyperparameters(BestModelClass, X_train_balanced, y_train_balanced)\n",
    "    y_pred_tuned = tuned_model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"EVALUATION\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    f1_tuned = f1_score(y_test, y_pred_tuned, average='weighted')\n",
    "    \n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Accuracy: {accuracy_tuned:.4f}\")\n",
    "    print(f\"  Weighted F1: {f1_tuned:.4f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_tuned, digits=4))\n",
    "    \n",
    "    baseline_model = BestModelClass(alpha=0.1)\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "    y_pred_baseline = baseline_model.predict(X_test)\n",
    "    \n",
    "    before_metrics = precision_recall_fscore_support(y_test, y_pred_baseline, \n",
    "                                                       labels=['negative', 'neutral', 'positive'])\n",
    "    after_metrics = precision_recall_fscore_support(y_test, y_pred_tuned,\n",
    "                                                      labels=['negative', 'neutral', 'positive'])\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    plot_confusion_matrix_comparison(y_test, y_pred_baseline, y_pred_tuned)\n",
    "    plot_f1_comparison(before_metrics, after_metrics)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"SAVING MODELS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    joblib.dump(tuned_model, OUTPUT_DIRS['models'] / 'naive_bayes_model_balanced.pkl')\n",
    "    joblib.dump(tfidf_vectorizer, OUTPUT_DIRS['models'] / 'tfidf_vectorizer.pkl')\n",
    "    \n",
    "    print(f\"✓ Model saved to {OUTPUT_DIRS['models'] / 'naive_bayes_model_balanced.pkl'}\")\n",
    "    print(f\"✓ Vectorizer saved to {OUTPUT_DIRS['models'] / 'tfidf_vectorizer.pkl'}\")\n",
    "    \n",
    "    df_test = df.loc[idx_test].copy()\n",
    "    df_test['predicted_sentiment'] = y_pred_tuned\n",
    "    df_test.to_csv(OUTPUT_DIRS['data'] / 'all_beauty_predictions_balanced.csv', index=False)\n",
    "    \n",
    "    print(f\"✓ Predictions saved to {OUTPUT_DIRS['data'] / 'all_beauty_predictions_balanced.csv'}\")\n",
    "    \n",
    "    save_smote_report(y_train, y_train_balanced, y_test, y_pred_baseline, y_pred_tuned)\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "\n",
    "\n",
    "def aspect_sentiment_analysis(df=None):\n",
    "    \"\"\"Perform aspect-based sentiment analysis.\"\"\"\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\nLoading predictions...\")\n",
    "        df = pd.read_csv(OUTPUT_DIRS['data'] / 'all_beauty_predictions_balanced.csv')\n",
    "    \n",
    "    print(f\"Total reviews: {len(df):,}\")\n",
    "    \n",
    "    df = df[df['topic_label'] != 'Unknown'].copy()\n",
    "    \n",
    "    aspect_sentiment = pd.crosstab(\n",
    "        df['topic_label'],\n",
    "        df['predicted_sentiment'],\n",
    "        normalize='index'\n",
    "    ) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"SENTIMENT DISTRIBUTION BY ASPECT\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"\\n\", aspect_sentiment.round(2))\n",
    "    \n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    plot_aspect_sentiment_distribution(df)\n",
    "    \n",
    "    aspect_sentiment_counts = pd.crosstab(df['topic_label'], df['predicted_sentiment'])\n",
    "    \n",
    "    aspect_summary = pd.DataFrame({\n",
    "        'Total_Reviews': df.groupby('topic_label').size(),\n",
    "        'Positive_%': aspect_sentiment['positive'],\n",
    "        'Neutral_%': aspect_sentiment['neutral'],\n",
    "        'Negative_%': aspect_sentiment['negative'],\n",
    "        'Net_Sentiment': aspect_sentiment['positive'] - aspect_sentiment['negative']\n",
    "    })\n",
    "    \n",
    "    aspect_summary = aspect_summary.sort_values('Net_Sentiment', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"ASPECT STRENGTHS & WEAKNESSES\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    print(\"\\nSTRONGEST ASPECTS:\")\n",
    "    for aspect in aspect_summary.head(3).index:\n",
    "        pos_pct = aspect_summary.loc[aspect, 'Positive_%']\n",
    "        neg_pct = aspect_summary.loc[aspect, 'Negative_%']\n",
    "        print(f\"  • {aspect}: {pos_pct:.1f}% positive, {neg_pct:.1f}% negative\")\n",
    "    \n",
    "    print(\"\\nWEAKEST ASPECTS:\")\n",
    "    for aspect in aspect_summary.tail(3).index:\n",
    "        pos_pct = aspect_summary.loc[aspect, 'Positive_%']\n",
    "        neg_pct = aspect_summary.loc[aspect, 'Negative_%']\n",
    "        print(f\"  • {aspect}: {pos_pct:.1f}% positive, {neg_pct:.1f}% negative\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"DETAILED ASPECT ANALYSIS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for aspect in aspect_summary.head(2).index:\n",
    "        analyze_aspect(df, aspect)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"GENERATING WORD CLOUDS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for aspect in aspect_summary.head(3).index:\n",
    "        print(f\"  Generating word clouds for {aspect}...\")\n",
    "        generate_aspect_wordclouds(df, aspect)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"TEMPORAL ANALYSIS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df['year_month'] = df['date'].dt.to_period('M')\n",
    "    \n",
    "    print(\"\\nPlotting overall sentiment trends...\")\n",
    "    plot_sentiment_trends(df)\n",
    "    \n",
    "    print(\"\\nPlotting aspect-specific trends...\")\n",
    "    for aspect in aspect_summary.head(2).index:\n",
    "        print(f\"  {aspect}...\")\n",
    "        plot_sentiment_trends(df, aspect)\n",
    "    \n",
    "    aspect_summary.to_csv(OUTPUT_DIRS['reports'] / 'aspect_summary.csv')\n",
    "    print(f\"\\n✓ Summary saved to {OUTPUT_DIRS['reports'] / 'aspect_summary.csv'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1005a52-bb93-4b0a-81c6-f0385548f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b51dc-b9cb-405e-b77a-78024ca01a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_explore('All_Beauty.jsonl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb23e9c-1582-439e-9b19-624ce3fba899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c7cc5-7d26-48c5-b059-7dacac991e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_topics = find_optimal_topics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31ba50-a33d-48d4-ae9c-908cee7f01f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = train_final_lda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd6e7d-11f9-41d6-9ade-89fbc0c9efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = assign_topics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff58b5e-2bb7-4dac-871f-a4c8141b2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = naive_bayes_classification(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff53833-a1b1-419d-9d32-941554fdd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_sentiment_analysis(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
